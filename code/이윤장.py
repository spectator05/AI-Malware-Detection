import os
import glob
import json
import pprint

# 추가됨
import csv

import numpy as np

from lightgbm import LGBMClassifier

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE

# 추가됨
from sklearn.feature_extraction import FeatureHasher


SEED = 41

def read_label_csv(path):
    label_table = dict()
    with open(path, "r") as f:
        for line in f.readlines()[1:]:          
            fname, label = line.strip().split(",")
            label_table[fname] = int(label)
    return label_table

def read_json(path):
    with open(path, "r") as f:
        return json.load(f)

def load_model(**kwargs):
    if kwargs["model"] == "rf":
        return RandomForestClassifier(random_state=kwargs["random_state"], n_jobs=4)
    elif kwargs["model"] == "dt":
        return DecisionTreeClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lgb":
        return LGBMClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "svm":
        return SVC(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lr":
        return LogisticRegression(random_state=kwargs["random_state"], n_jobs=-1)
    elif kwargs["model"] == "knn":
        return KNeighborsClassifier(n_jobs=-1)
    elif kwargs["model"] == "adaboost":
        return AdaBoostClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "mlp":
        return MLPClassifier(random_state=kwargs["random_state"])
    else:
        print("Unsupported Algorithm")
        return None
    

def train(X_train, y_train, model):
    '''
        머신러닝 모델을 선택하여 학습을 진행하는 함수
	
        :param X_train: 학습할 2차원 리스트 특징벡터
        :param y_train: 학습할 1차원 레이블 벡터
        :param model: 선택할 머신러닝 알고리즘
        :return: 학습된 머신러닝 모델
    '''
    clf = load_model(model=model, random_state=SEED)
    clf.fit(X_train, y_train)
    return clf


def evaluate(X_test, y_test, model):
    '''
        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수
	
        :param X_test: 검증할 2차원 리스트 특징 벡터
        :param y_test: 검증할 1차원 리스트 레이블 벡터
        :param model: 학습된 머신러닝 모델 객체
    '''
    predict = model.predict(X_test)
    print(model, "정확도", model.score(X_test, y_test), end=" | ")


# PEMINER의 json파일에서 모든 데이터를 추출(188개)
class PeminerParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []
    
    def process_report(self):
        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]
        return self.vector
    
# EMBER의 json파일에서 데이터 추출, process_report의 인자로 선택 가능(<= 2381개)
# ember.py 참고
class EmberParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []
    
    def get_histogram_info(self):
        histogram = np.array(self.report["histogram"])
        total = histogram.sum()
        vector = histogram / total
        return vector.tolist()
    
    def get_byteentropy_info(self):
        byteentropy = np.array(self.report["byteentropy"])
        sum = byteentropy.sum()
        vector = []
        vector.extend(byteentropy / sum)
        return vector

    def get_string_info(self):
        strings = self.report["strings"]

        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0
        vector = [
            strings['numstrings'], 
            strings['avlength'], 
            strings['printables'],
            strings['entropy'], 
            strings['paths'], 
            strings['urls'],
            strings['registry'], 
            strings['MZ']
        ]
        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()
        return vector
    
    def get_general_file_info(self):
        general = self.report["general"]
        vector = [
            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],
            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],
            general['symbols']
        ]
        return vector

    def get_section_info(self):
        section = self.report["section"]
        sections = section["sections"]
        vector = [
            len(sections),
            sum(1 for s in sections if s["size"] == 0),
            # number of sections with an empty name
            sum(1 for s in sections if s["name"] == ""),
            # number of RX
            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),
            # number of W
            sum(1 for s in sections if 'MEM_WRITE' in s['props'])
        ]

        section_sizes = [(s['name'], s['size']) for s in sections] 
        vector.extend(FeatureHasher(50, input_type="pair").transform([section_sizes]).toarray()[0]) # section_sizes_hashed
        section_entropy = [(s['name'], s['entropy']) for s in sections]
        vector.extend(FeatureHasher(50, input_type="pair").transform([section_entropy]).toarray()[0]) # section_entropy_hashed
        section_vsize = [(s['name'], s['vsize']) for s in sections]
        vector.extend(FeatureHasher(50, input_type="pair").transform([section_vsize]).toarray()[0]) # section_vsize_hashed
        vector.extend(FeatureHasher(50, input_type="string").transform([section['entry']]).toarray()[0]) # entry_name_hashed
        characteristics = [p for s in sections for p in s['props'] if s['name'] == section['entry']]
        vector.extend(FeatureHasher(50, input_type="string").transform([characteristics]).toarray()[0]) # characteristics_hashed
        return vector
    
    def get_imports_info(self):
        data = self.report["imports"]
        vector = []
        libraries = list(set([l.lower() for l in data]))
        vector.extend(FeatureHasher(256, input_type="string").transform([libraries]).toarray()[0]) # libraries_hashed

        imports = [lib.lower() + ':' + e for lib, elist in data.items() for e in elist]
        vector.extend(FeatureHasher(1024, input_type="string").transform([imports]).toarray()[0]) # imports_hashed

        return vector

    def get_exports_info(self):
        exports = self.report["exports"]
        vector = []
        vector.extend(FeatureHasher(128, input_type="string").transform([exports]).toarray()[0]) # exports_hashed
        return vector

    def get_header_file_info(self):
        header = self.report["header"]
        vector = [header["coff"]["timestamp"]]
        vector.extend(FeatureHasher(10, input_type="string").transform([[header['coff']['machine']]]).toarray()[0])
        vector.extend(FeatureHasher(10, input_type="string").transform([header['coff']['characteristics']]).toarray()[0])
        vector.extend(FeatureHasher(10, input_type="string").transform([[header['optional']['subsystem']]]).toarray()[0])
        vector.extend(FeatureHasher(10, input_type="string").transform([header['optional']['dll_characteristics']]).toarray()[0])
        vector.extend(FeatureHasher(10, input_type="string").transform([[header['optional']['magic']]]).toarray()[0])
        vector.extend([header['optional']['major_image_version'],
            header['optional']['minor_image_version'],
            header['optional']['major_linker_version'],
            header['optional']['minor_linker_version'],
            header['optional']['major_operating_system_version'],
            header['optional']['minor_operating_system_version'],
            header['optional']['major_subsystem_version'],
            header['optional']['minor_subsystem_version'],
            header['optional']['sizeof_code'],
            header['optional']['sizeof_headers'],
            header['optional']['sizeof_heap_commit'],
            ])
        return vector

    def get_data_directories_info(self):
        datadirectories = self.report["datadirectories"]
        name_order = [
            "EXPORT_TABLE", "IMPORT_TABLE", "RESOURCE_TABLE", "EXCEPTION_TABLE", "CERTIFICATE_TABLE",
            "BASE_RELOCATION_TABLE", "DEBUG", "ARCHITECTURE", "GLOBAL_PTR", "TLS_TABLE", "LOAD_CONFIG_TABLE",
            "BOUND_IMPORT", "IAT", "DELAY_IMPORT_DESCRIPTOR", "CLR_RUNTIME_HEADER"
        ]
        features = np.zeros(2 * len(name_order), dtype=np.float32)
        for i in range(len(name_order)):
            if i < len(datadirectories):
                features[2 * i] = datadirectories[i]["size"]
                features[2 * i + 1] = datadirectories[i]["virtual_address"]
        return features.tolist()
 
    # 원하는 함수만 선택하여 특징 벡터에 추가할 수 있게 함
    def process_report(self, features):
        vector = []
        if "general" in features:
            vector += self.get_general_file_info()
        if "histogram" in features:
            vector += self.get_histogram_info()
        if "string" in features:
            vector += self.get_string_info()
        if "byteentropy" in features:
            vector += self.get_byteentropy_info()
        if "section" in features:
            vector += self.get_section_info()
        if "imports" in features:
            vector += self.get_imports_info()
        if "exports" in features:
            vector += self.get_exports_info()
        if "header_file" in features:
            vector += self.get_header_file_info()       
        if "data_directories" in features:
            vector += self.get_data_directories_info()
        return vector 

# EMBER의 json파일에서 특정 데이터 추출(21개)
# 유의미한 문자열 데이터가 거의 없다고 판단하여 FeatureHasher를 사용한 문자열 추출은 하지 않음
class PestudioParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []
    
    # overview의 수치화 된 데이터 일부 추출
    def get_overview_info(self):
        image = self.report["image"]
        overview = image["overview"]
        vector = [float(overview['entropy']),
                  int(0 if overview['size'] == "n/a" or "empty" else overview['size']),
                  int(0 if overview['size-without-overlay'] == "n/a" or "empty" else overview['size-without-overlay']),
                  int(0 if overview['compiler-stamp'] == "n/a" or "empty" else overview['compiler-stamp'].split(" ")[-1]),
                  int(0 if overview['debugger-stamp'] == "n/a" or "empty" else overview['debugger-stamp'].split(" ")[-1]),
                  int(0 if overview['resources-stamp'] == "n/a" or "empty" else overview['resources-stamp'].split(" ")[-1]),
                  int(0 if overview['exports-stamp'] == "n/a" or "empty" else overview['exports-stamp'].split(" ")[-1]),
                  int(0 if overview['version-stamp'] == "n/a" or "empty" else overview['version-stamp'].split(" ")[-1])
                 ]
        return vector

    # incidator의 severity를 기준으로 severity가 1인 비율을 추출
    def get_indicator_info(self):
        indicators = self.report["image"]["indicators"]
        try:
            vector = [sum(1 for s in indicators["indicator"] if s["@severity"] == "1") / len(indicators["indicator"])]
        except KeyError:
            vector = [-1]
        return vector
    
    # virustotal의 수치화된 데이터 일부 추출
    def get_virustotal_info(self):
        virustotal = self.report["image"]["virustotal"]
        vector = [0 if virustotal == "offline" or "n/a" else int(virustotal["@detection"]),
                  0 if virustotal == "offline" or "n/a" else int(virustotal["@engine"])
                 ]
        return vector
    
    # dos-header의 크기 추출
    def get_dosheader_info(self):
        try:
            header = self.report["image"]["dos-header"]
        except KeyError:
            return [-1]
        vector = [int(header["size"])]
        return vector
    
    # dos-stub의 크기 추출
    def get_dosstub_info(self):
        try:
            dos_stub = self.report["image"]["dos-stub"]
        except KeyError:
            return [-1]
        vector = [int(dos_stub["size"])]
        return vector
    
    # file-header의 compiler-stamp의 연도만 추출
    def get_fileheader_info(self):
        try:
            header = self.report["image"]["file-header"]
        except KeyError:
            return [-1]
        vector = [int(-1 if header["compiler-stamp"] == "n/a" or "empty" else header["compiler-stamp"].split(" ")[-1]),]
        return vector          

    # optional-header의 file-checksum을 16진수 -> 10진수 변환 추출
    def get_optionalheader_info(self):
        try:
            header = self.report["image"]["optional-header"]
        except KeyError:
            return [-1]
        vector = [int(header["file-checksum"], 16)]
        return vector
    
    # directories의 @size가 0이 아닌 요소들의 합 추출
    def get_directories_info(self):
        try:
            directory = self.report["image"]["directories"]["directory"]
        except KeyError:
            return [-1]
        vector = [sum(1 for d in directory if d["@size"] != "0")]
        return vector

    # libraries의 @blacklist = "x"인 요소들의 비율 추출
    def get_libraries_info(self):
        try:
            libraries = self.report["image"]["libraries"]
        except KeyError:
            return [-1]
        if libraries == "n/a" or len(libraries) == 1:
            return [0]
        try:
            vector = [sum(1 for i in libraries["library"] if i["@blacklist"] == "x") / len(libraries["library"])]
        except TypeError:
            vector = [1 if libraries["library"]["@blacklist"] == "x" else 0]
        return vector

    # imports의 @blacklist = "x"인 요소들의 비율 추출
    def get_imports_info(self):
        try:
            imports = self.report["image"]["imports"]
        except KeyError:
            return [-1]
        if imports == "n/a" or len(imports) == 1:
            return [0]
        try:
            vector = [sum(1 for i in imports["import"] if i["@blacklist"] == "x") / len(imports["import"])]
        except TypeError:
            vector = [1 if imports["import"]["@blacklist"] == "x" else 0]
        return vector
    
    # exports의 export 요소 개수 추출
    def get_exports_info(self):
        try:
            exports = self.report["image"]["exports"]
        except KeyError:
            return [-1]
        if exports == "n/a":
            return [0]
        vector = [len(exports["export"])]
        return vector

    # strings의 @bl, @count 추출
    def get_strings_info(self):
        strings = self.report["image"]["strings"]
        vector = [float(strings["@bl"]), float(strings["@count"])]
        return vector
    
    def process_report(self):
        vector = []
        vector += self.get_overview_info()
        vector += self.get_indicator_info()
        vector += self.get_virustotal_info()
        vector += self.get_dosheader_info()
        vector += self.get_dosstub_info()
        vector += self.get_fileheader_info()
        vector += self.get_optionalheader_info()
        vector += self.get_directories_info()
        vector += self.get_libraries_info()
        vector += self.get_imports_info()
        vector += self.get_exports_info()
        vector += self.get_strings_info()
        return vector

ember_dir = "./데이터/EMBER/학습데이터/"
peminer_dir = "./데이터/PEMINER/학습데이터/"
pestudio_dir = "./데이터/PESTUDIO/학습데이터/"
val_peminer_dir = "./데이터/PEMINER/검증데이터/"
val_ember_dir = "./데이터/EMBER/검증데이터/"
val_pestudio_dir = "./데이터/PESTUDIO/검증데이터/"
test_peminer_dir = "./데이터/PEMINER/테스트데이터/"
test_ember_dir = "./데이터/EMBER/테스트데이터/"
test_pestudio_dir = "./데이터/PESTUDIO/테스트데이터/"
label_table = read_label_csv("./데이터/학습데이터_정답.csv")
val_label_table = read_label_csv("./데이터/검증데이터_정답.csv")
peminer_path = os.listdir(peminer_dir)
val_peminer_path = os.listdir(val_peminer_dir)
test_peminer_path = os.listdir(test_peminer_dir)


y = []

for fname in peminer_path[:20000]:
    label = label_table[fname[:-5]]
    y.append(label)

import copy

e_features = [['imports', 'section', 'general'],
              ['imports', 'section', 'general', 'data_directories'],
              ['imports', 'section', 'exports'],
              ['imports', 'section', 'data_directories'],
              ['imports', 'section']
             ]

# 정답레이블데이터 : 2만 개(`y`) + 1만개
vest_y = copy.deepcopy(y)
i = 0
for fname in val_peminer_path[:10000]:
    label = val_label_table[fname[:-5]]
    vest_y.append(label)

# 학습데이터 : 학습2만 개 + 검증1만 개
vest_X = []
count = 0
for fname in peminer_path[:20000]:
    count += 1
    if count % 1000 == 0:
        print(count)
    label = label_table[fname[:-5]]
    parsed_feature = []
    parsed_feature += PeminerParser(f"{peminer_dir}{fname[:-5]}.json").process_report()
    parsed_feature += EmberParser(f"{ember_dir}{fname[:-5]}.json").process_report(e_features[3])
    try:
        parsed_feature += PestudioParser(f"{pestudio_dir}{fname[:-5]}.json").process_report()
    except FileNotFoundError:
        parsed_feature += [-1 for _ in range(21)]
    vest_X.append(parsed_feature)

count = 20000
for fname in val_peminer_path[:10000]:
    count += 1
    if count % 1000 == 0:
        print(count)
    parsed_feature = []
    parsed_feature += PeminerParser(f"{val_peminer_dir}{fname[:-5]}.json").process_report()
    parsed_feature += EmberParser(f"{val_ember_dir}{fname[:-5]}.json").process_report(e_features[3])
    try:
        parsed_feature += PestudioParser(f"{val_pestudio_dir}{fname[:-5]}.json").process_report()
    except FileNotFoundError:
        parsed_feature += [-1 for _ in range(21)]
    vest_X.append(parsed_feature)

def save_csv(filename, predict):
    f = open('./predict.csv', 'w', encoding='utf-8', newline='')
    wr = csv.writer(f)
    wr.writerow(['file', 'predict'])

    for idx in range(len(filename)):
        wr.writerow([filename[idx][:-5], predict[idx]])

    f.close()


def save_ensemble_result(X, models, path=[]):

    # Soft Voting
    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting
    predicts = []
    count = 0
    for model in models:
        prob = [result for _, result in model.predict_proba(X)]
        predicts.append(prob)
    
    predict = np.mean(predicts, axis=0)
    predict = [1 if x >= 0.5 else 0 for x in predict]
    save_csv(path, predict)



test_X = []
count = 0
for fname in test_peminer_path[:10000]:
    count += 1
    if count % 1000 == 0:
        print(count)
    path = f"{val_pestudio_dir}{fname[:-5]}.json"
    parsed_feature = []
    parsed_feature += PeminerParser(f"{test_peminer_dir}{fname[:-5]}.json").process_report()
    parsed_feature += EmberParser(f"{test_ember_dir}{fname[:-5]}.json").process_report(e_features[3])
    try:
        parsed_feature += PestudioParser(f"{test_pestudio_dir}{fname[:-5]}.json").process_report()
    except FileNotFoundError:
        parsed_feature += [-1 for _ in range(21)]
    test_X.append(parsed_feature)

# 3만개 학습 모델
models = []
for model in ["rf", "lgb"]:
    clf = train(vest_X, vest_y, model)
    models.append(clf)

save_ensemble_result(test_X, models, test_peminer_path)

